{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://weclouddata.com/wp-content/uploads/2016/11/logo.png' width='30%'>\n",
    "-------------\n",
    "\n",
    "<h3 align='center'> Applied Machine Learning Course - Assignment Week 2 </h3>\n",
    "<h1 align='center'> Iris Dataset Classification </h1>\n",
    "\n",
    "<br>\n",
    "<center align=\"left\"> Developed by:</center>\n",
    "<center align=\"left\"> WeCloudData Academy </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "In this assignment, we will practice some advanced Logistic Regression technique on the famous [iris dataset](#https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "This is perhaps the best known database to be found in the machine learning literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. \n",
    "\n",
    "Predicted attribute: **class of iris plant**. \n",
    "\n",
    "This is an exceedingly simple domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Description</h2>\n",
    "\n",
    "<b>Features:</b>\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "\n",
    "<b>Target Value:</b>\n",
    "\n",
    "- class: \n",
    "  1. Iris Setosa \n",
    "  2. Iris Versicolour \n",
    "  3. Iris Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ 1: Explore the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (numpy, sklearn, matplotlib) and \n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Step 1</b> \n",
    "\n",
    "  Load the iris dataset from sklearn into a variable called `data`.\n",
    "\n",
    "  - Hint: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris\n",
    "  - We have used similar technique to load the wine dataset in the in-class lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Step 2</b> \n",
    "\n",
    "  Explore the dataset\n",
    "\n",
    "  - Figure out how many features there are and how many classes there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Step 3</b> \n",
    "\n",
    "  Explore class distribution\n",
    "\n",
    "  - Find out how many samples there are for each target class.\n",
    "  - Visualize 2D feature distribution on this dataset (hint: we used similar techniques in the in-class lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n",
    "def visualize_2d(feature_indices, all_feature_names, target_names, X, y):\n",
    "    # TODO: implement the similar visualization routine as we saw in the lab\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pick two features and visualize the data distribution w.r.t. the chosen two features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the distribution of different classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ 2: Prepare Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 1</b>\n",
    "\n",
    "  - Use the 'train_test_split' function in scikit learn to split X and y into 80% Traning data and 20% Testing Data\n",
    "  \n",
    "  - Hint: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 2</b>\n",
    "\n",
    "  - Perform feature standardization on `X_train` by using sklearn's `StandardScaler`, and use the same standardizer to standardize `X_test`.\n",
    "  - Hint: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 3</b>\n",
    "\n",
    "  - Repeat the data visualization you did in `Step 3: Explore class distribution` in the first section above on the training and test data (`X_train` and `X_test`) after the standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ 3: Multi-class Logistic Regression </h3>\n",
    "\n",
    "We have practiced about performing binary classification using Logistic Regression. However, Logistic Regression can also perform multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 1</b>\n",
    "  - Create a Logistic Regression model - 'lr' and fit X_train and Y_train to train it. Do you think there is any special setup you need to do for making this Logistic Regression classifier capable of performing multi-class classification?\n",
    "  - Hint: Look at the function doc at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 2</b>\n",
    "  - Import classification_report and accuracy_score from `sklearn.metrics.classification` to evaluate our classifier\n",
    "  - Use `lr.predict` on `X_test` to get predicted value of `y` and call `classification_report(y_test, y_predict)` to get generate the classification report.\n",
    "  - Call `accuracy_score` to get the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 3</b>\n",
    "  - What's your model's performance (using the same set of metrics as in Step 2) on the `training` data? This is to understand whether our model is underfitting or not, i.e., if the training performance is not almost perfect, it is an indicator that our model is not expressive enough to represent the variance in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ 4: Model generality </h3>\n",
    "\n",
    "Logistic Regression in scikit-learn has regularization built in. It comes with many different variants, depending on the particular optimization solver and so on. But roughly speaking, there is a parameter $C$ which controls the strength of the regularization.\n",
    "\n",
    "According to scikit-learn's documentation, this parameter $C$ is:\n",
    "\n",
    "``` Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.```\n",
    "\n",
    "Roughly speaking, the cost function has either one of the following forms:\n",
    "\n",
    "<center>\n",
    "\n",
    "$J_\\theta=\\sum_{i=1}^{m} (\\hat{y}^{(i)}-y^{(i)}) + \\frac{1}{C} \\times ||{\\theta}||$ (using l1 norm), or\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "$J_\\theta=\\sum_{i=1}^{m} (\\hat{y}^{(i)}-y^{(i)}) + \\frac{1}{C} \\times ||{\\theta}||^2$ (using l2 norm)\n",
    "</center>\n",
    "\n",
    "In this exercise, we will experiment with using different $C$ values, and find the optimal $C$ value on a validation set. Then we use the model built with this optimal $C$ value to perform prediction on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Step 1: </b>\n",
    "  - Further split the current training set to be 80% actual training set (`X_t` and `y_t`) and 20% validation set (`X_val` and `y_val`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Step 2: </b>\n",
    "  - For each $C$ value in this set: $[0.001, 0.01, 0.1, 1, 5, 10, 50, 100]$, fit a LogisticRegression model `lr` with this $C$ value on `X_t` and `y_t`, and use `lr` to predict `X_val`. Keep track of  the accuracy score on the training set (`X_t` and `y_t`) and validation set (`X_val` and `y_val`).\n",
    "  - Report the $C$ value which achieves the best validation accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "candidate_c = [0.001, 0.01, 0.1, 1, 5, 10, 50, 100]\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# TODO: compute the training and validation accuracy scores over different $C$ values\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# sort val_accs to find the best val accuracy and the corresponding c value which achieves the best val accuracy\n",
    "best_c_idx, best_val_acc = sorted(enumerate(val_accs), key=lambda x : x[1], reverse=True)[0]\n",
    "best_c = candidate_c[best_c_idx]\n",
    "\n",
    "print(f'Best validation accuracy %.1f is achieved using c=%.3f' % (best_val_acc * 100.0, best_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Step 3</b>\n",
    "  - Plot a curve of training accuracies vs. different $C$ value and a similar curve of validation accuracies vs. different $C$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 4</b>\n",
    "\n",
    "  Use the optimal value of $C$ you found in step 2 to train a new Logistic Regression classifier `lr` on the combination of `X_t` and `X_val` (i.e., the original `X_train`) and test your new model on the test data `X_test` and `y_test`. Report the performance now and compare with that you obtained in [Multi-class Logistic Regreesion]($\\Omega$-3:-Multi-class-Logistic-Regression-) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ 5 (Advanced): Implement gradient descent on Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Under the hood, Scikit-learn is already using regularization (via $C$ value) and gradient descent based method to to fit LogisticRegression.\n",
    "\n",
    "- Therefore, you have been enjoying the simple life of fitting a LogisticRegression classifier by calling sklearn directly. \n",
    "\n",
    "- In this step, you can challenge yourself to implement the most vanilla version of training a logistic regression by gradient descent, i.e., **no regularization**.\n",
    "\n",
    "- However, this dataset is a **multi-class** dataset, and it requires special care to perform multi-class classification. Therefore, let's first transform this dataset into a binary classification dataset by merging the last two classes. The code for this part is provided to you.\n",
    "\n",
    "- For more info on multi-class logistic regression, see https://cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.4-MultiLogistic.pdf![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in [y_train, y_test]:\n",
    "    class_2_idx = np.where(y==2) # the indices of the samples where the original class is 2\n",
    "    y[class_2_idx] = 1 # assign these samples a new class label: 1, i.e., they are merged into the class where labels=1\n",
    "\n",
    "# confirm there are only two classes in `y_train` and `y_test` now:\n",
    "print(f'unique values in y_train: {np.unique(y_train)}')\n",
    "print(f'unique values in y_test: {np.unique(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 1: </b>\n",
    "\n",
    "  Initialize all parameters randomly\n",
    "\n",
    "<center>\n",
    "$\\hat{y}=\\sigma({\\theta^T} x)=\\cfrac{1}{e^{(-\\theta^Tx)}}$\n",
    "<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def initialize_theta(dim):\n",
    "    import numpy.random\n",
    "    # TODO: randomly initialize coefficient to be a dim-sized 1D vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 2</b>\n",
    "\n",
    "  - Calculate the gradient of the logistic regresion function $\\sigma(\\theta^Tx)$ against each $\\theta_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: implement this function\n",
    "import numpy as np\n",
    "\n",
    "def prediction(X, current_theta):\n",
    "    # TODO: compute the current estimation of the output, H, given the current_theta and X\n",
    "    # X.shape: m*(p+1)\n",
    "    # current_theta.shape: (p+1)*1\n",
    "    \n",
    "    \n",
    "    \n",
    "def loss(X, y, current_theta):\n",
    "    H = prediction(X, current_theta) # H.shape: m*1\n",
    "    # TODO: compute loss function J, given H, and y\n",
    "    \n",
    "    \n",
    "    \n",
    "def loss_gradient(X, y, current_theta):\n",
    "    # TODO: implement the loss gradient\n",
    "    # compute the current estimation of the output, H, given the current_theta and X\n",
    "    # compute loss function J, given H, and y\n",
    "    # compute gradient of this loss function against current_theta\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 3</b>\n",
    "\n",
    "  - Verify your gradient computation\n",
    "  \n",
    "  According to the definition of gradient: \n",
    "  <center>\n",
    "  $\\frac{\\partial{J}}{\\partial{\\theta}}=(\\frac{\\partial{J}}{\\partial{\\theta_0}},\\frac{\\partial{J}}{\\partial{\\theta_1}},\\ldots,\\frac{\\partial{J}}{\\partial{\\theta_p}})$\n",
    "  </center>\n",
    "  \n",
    "  That is, the gradient is just the generalization of derivatives to the multivariate situation.\n",
    "  \n",
    "  For derivatives, we know it is defined as:\n",
    "  \n",
    "  <center>\n",
    "  $\\frac{dJ(\\theta)}{d\\theta}=\\lim_{\\Delta\\rightarrow 0}\\frac{J(\\theta+\\delta)-J(\\theta-\\delta)}{2\\delta}$,\n",
    "  </center>\n",
    "  \n",
    "  i.e., it can be computed by taking a really small step of $\\Delta$ from the original value of $\\theta$ and compute the difference between the new value of $J=J(\\theta+\\Delta)$ and the original value $J(\\theta)$, then divided by that very small step $\\Delta$.\n",
    "  \n",
    "  Similarly to derivatives, gradient can be computed in a similar fashion:\n",
    "  \n",
    "    <center>\n",
    "  $\\frac{\\partial{J}}{\\partial{\\theta}}=(\\frac{\\partial{J}}{\\partial{\\theta_0}},\\frac{\\partial{J}}{\\partial{\\theta_1}},\\ldots,\\frac{\\partial{J}}{\\partial{\\theta_p}})$\n",
    "  <br>\n",
    "  \n",
    "  $\\frac{\\partial{J(\\theta)}}{\\partial{\\theta_j}}=\\lim_{\\Delta\\rightarrow 0}\\frac{J(\\theta_j+\\delta)-J(\\theta_j-\\delta)}{2\\delta}$, for $j=0,1,\\ldots,p$\n",
    "  </center>\n",
    "  \n",
    "  Therefore, we can verify the correctness of your computed gradients by checking whether it is approximately the value of $\\frac{J(\\theta_j+\\delta)-J(\\theta_j-\\delta)}{2\\delta}$ when $\\delta$ is a very small number.\n",
    "  \n",
    "  Here, we provide the verification code for you, but please study the code carefully to understand the idea behind it. If, in training time later, your computed gradients (the `loss_gradient` function defined ealier) do not pass this verification code, it means your implementation of the `loss_gradient` function is very likely wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def check_loss_gradient(X, y, current_theta):\n",
    "    gradients = loss_gradient(X, y, current_theta)\n",
    "    # gradients: a 1-D vector with p+1 values, i.e., gradients[i] is the i-th partial derivative, \n",
    "    \n",
    "    delta = 1e-10\n",
    "    \n",
    "    current_loss = loss(X, y, current_theta)\n",
    "    \n",
    "    approx_gradients = []\n",
    "    for j in range(len(current_theta)):\n",
    "        forward_theta = deepcopy(current_theta) # we cannot use `forward_theta=current_theta`\n",
    "        # as this will be a shallow copy, and when we modify `forward_theta`, the original current_theta\n",
    "        # will be modified as well\n",
    "        \n",
    "        forward_theta[j] = forward_theta[j] + delta # we only add delta to the j-th parameter\n",
    "        backward_theta = deepcopy(current_theta)\n",
    "        backward_theta[j] -= delta # we only subtract delta to the j-th parameter\n",
    "        \n",
    "        forward_loss = loss(X, y, forward_theta)\n",
    "        backward_loss = loss(X, y, backward_theta)\n",
    "        approx_gradient_j = (forward_loss - backward_loss) / (2*delta)\n",
    "        \n",
    "        approx_gradients.append(approx_gradient_j)\n",
    "    \n",
    "    return np.allclose(gradients, approx_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, you should be seeing `True` returned by calling:\n",
    "check_loss_gradient(X=X_train, y=y_train, current_theta=initialize_theta(dim=X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 3</b>\n",
    "\n",
    "  - Define a learning rate $\\alpha$, which would be the step size to update each $\\theta_i$, $\\theta_i=\\theta_i - \\alpha \\times loss\\_gradient_i$. Repeatedly updating all $\\theta_i$'s until the loss converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4:\n",
    "\n",
    "def update_theta(gradient, current_theta, step_size):\n",
    "    # TODO: implement theta update logic\n",
    "     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put the skeleton code here for you\n",
    "\n",
    "# initialization:\n",
    "# convert pandas dataframe into numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "precision = 0.001\n",
    "step_size = 0.1 # use your own step_size\n",
    "current_theta = initialize_theta(dim=X_train.shape[1]) # you need to determine the input value to `dim`\n",
    "current_loss = loss(X_train, y_train, current_theta)\n",
    "losses = [current_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main graident descent loop\n",
    "while len(losses) < 2 or abs(losses[-1] - losses[-2]) > precision: # all some other convergence condition\n",
    "    gradient = loss_gradient(X_train, y_train, current_theta)\n",
    "    if not check_loss_gradient(gradient, X, current_theta):\n",
    "        print('Check your gradient implementation!')\n",
    "        break\n",
    "    \n",
    "    current_theta = update_theta(gradient, current_theta, step_size)\n",
    "    \n",
    "    # compute current loss\n",
    "    current_loss = loss(X_train, y_train, current_theta)\n",
    "    losses.append(current_loss)\n",
    "    \n",
    "# once converged, current_theta are therefore the coefficients in the linear regression model  \n",
    "print(f'converge after {len(losses)} iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss against number of iterations\n",
    "# you should be seeing the cost value gradually decrease, if not, your implmenetation is wrong\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(losses)), losses)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Step 5</b>\n",
    "\n",
    "  - Calculate the classification evaluation metrics (precision, recall, f1, and accuracy) when applying your model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
