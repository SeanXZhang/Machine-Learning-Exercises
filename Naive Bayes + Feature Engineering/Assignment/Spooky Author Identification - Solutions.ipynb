{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://weclouddata.com/wp-content/uploads/2016/11/logo.png' width='30%'>\n",
    "-------------\n",
    "\n",
    "<h3 align='center'> Applied Machine Learning Course - Assignment Week 3 </h3>\n",
    "<h1 align='center'> Spooky Author Identification </h1>\n",
    "\n",
    "<br>\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "In this assignment, we will practice using Naive Bayes and Logistic Regression to perform authorship identification. \n",
    "\n",
    "The dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer (an automatic tokenizer similar to the NLTK one we used in the lab), so you may notice the odd non-sentence here and there. Your objective is to accurately identify the author of the sentences in the test set.\n",
    "\n",
    "You will need to use the text preprocessing techniques we learned in the lab to transform the text data.\n",
    "\n",
    "> We will use the `train.csv` for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Description</h2>\n",
    "\n",
    "There are only three columns in the CSV file: \n",
    "- \"**id**\": The id of the sample. You should not use this column.\n",
    "- \"**text**\": The sentence that you will need to build your model with.\n",
    "- \"**author**\": The target you need to predict.\n",
    "  1. _EAP_: Edgar Allan Poe\n",
    "  2. _HPL_: HP Lovecraft\n",
    "  3. _MWS_: Mary Shelley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 1: Explore the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Load the training data `train.csv` into a Dataframe named `data`.\n",
    "\n",
    "Import the necessary packages if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:24:19.008035Z",
     "start_time": "2019-07-13T21:24:18.892029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Separate the dataframe into the data (the \"text\" column) and the targets (the \"author\" column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:24:35.394973Z",
     "start_time": "2019-07-13T21:24:35.390972Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "  \n",
    "Explore the training data using any technique you have learned so far.\n",
    "\n",
    "For example, check the distribution of different authors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T20:45:57.808414Z",
     "start_time": "2019-07-13T20:45:57.790413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>7900</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>5635</td>\n",
       "      <td>5635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>6044</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  text\n",
       "author            \n",
       "EAP     7900  7900\n",
       "HPL     5635  5635\n",
       "MWS     6044  6044"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('author').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T20:47:14.735814Z",
     "start_time": "2019-07-13T20:47:14.609807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.730476530977068"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.split(' ').map(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T20:48:02.721559Z",
     "start_time": "2019-07-13T20:48:02.591551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.split(' ').map(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T20:48:07.538834Z",
     "start_time": "2019-07-13T20:48:07.349824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.split(' ').map(len).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 2: Split data into training and testing\n",
    "\n",
    "Let us do our old trick of splitting the data into 80% for training and 20% for testing.\n",
    "\n",
    "Call the resulting training and test data as `X_train` and `X_test`; the resulting training and test targets as `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:27:28.605880Z",
     "start_time": "2019-07-13T21:27:28.596879Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: do the 80-20 train/test splitting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 3: Preprocess text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Write a function to tokenize the text into words. We assume each text is one single sentence, so there is no need to do sentence splitting here.\n",
    "\n",
    "> Hint: use NLTK's `word_tokenize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:28:42.668116Z",
     "start_time": "2019-07-13T21:28:42.663115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import nltk\n",
    "\n",
    "def tokenize_words(text):\n",
    "    # TODO tokenize the text into words\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Remove stopwords from the text.\n",
    "\n",
    "> Hint: use stopwords from NLTK to do the filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T20:59:28.838802Z",
     "start_time": "2019-07-13T20:59:28.826802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # TODO: return a new list of words with any stopwords removed\n",
    "    return [word.lower() for word in words if word.lower() not in en_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Combine the first two steps to perform end-to-end preprocessing for a given text. We have done this part for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:31:42.437398Z",
     "start_time": "2019-07-13T21:31:42.432398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "\n",
    "def preprocess(text):\n",
    "    words = tokenize_words(text) # tokenize\n",
    "    words = remove_stopwords(words) # remove stopwords\n",
    "    \n",
    "    # combine the words together into one single string, \n",
    "    # such that the down-stream vectorizer can work with\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Test your `preprocess` function in the previous step by running through some text and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:31:55.681156Z",
     "start_time": "2019-07-13T21:31:55.676155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native sprightliness needed undue excitement\n"
     ]
    }
   ],
   "source": [
    "print(preprocess('Her native sprightliness needed no undue excitement'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Use `preprocess()` function defined in Step 3 to process each text in `X_train`. Do the same for `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:32:24.887826Z",
     "start_time": "2019-07-13T21:32:18.656470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5\n",
    "X_train = [preprocess(x) for x in X_train]\n",
    "\n",
    "# TODO: preprocess X_test in a similar way\n",
    "X_test = [preprocess(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 3: Vectorize the text data\n",
    "\n",
    "After the preprocessing, we need to vectorize, i.e., transform the words into actual numeric numbers that the model can digest. \n",
    "\n",
    "For text data, [Tf-idf vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) is a good choice, as it is able to represent each word using its TF-IDF weights.\n",
    "\n",
    "> Hint: We talked about `TF-IDF` weights in the class, which is an empirically solid measurement of word importance in text.\n",
    "\n",
    "> For more info, look at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Initialize an Tf-Idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:33:21.236049Z",
     "start_time": "2019-07-13T21:33:21.230049Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "# by setting `min_df=2`, we will ignore any words with less than 2 occurrences in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Fit the Tf-IDF vectorizer on our **training data**.\n",
    "\n",
    "> Hint: use the `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:33:35.909888Z",
     "start_time": "2019-07-13T21:33:35.478864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# TODO: use the vectorizer defined above to fit and transform the training data `X_train`\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Transform the **test data** using the vectorizer fitted in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:35:03.942923Z",
     "start_time": "2019-07-13T21:35:03.842918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# TODO: use the vectorizer fitted in the previous step to transform the test data `X_test`\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 4: Train a Naive Bayes classifier\n",
    "\n",
    "In lab we implemented our own Naive Bayes classifier. Here, we will use Sklearn's [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) to train our authorship classifier.\n",
    "\n",
    "> For more info: see http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Initialize a MultinomialNB classifier object and call it `nb`. \n",
    "\n",
    "Read its documentation (http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) to figure out if you want to use any non-default values when initializing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:35:17.046673Z",
     "start_time": "2019-07-13T21:35:17.041673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# TODO: initialize a MultinomialNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Fit the classifier `nb` using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:35:27.717283Z",
     "start_time": "2019-07-13T21:35:27.641279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2\n",
    "# TODO: fit the NB classifier on training data\n",
    "\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Use the trained `nb` classifier to classify the texts in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:35:29.936410Z",
     "start_time": "2019-07-13T21:35:29.931410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# TODO: predict test data\n",
    "\n",
    "y_preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Write a function to evaluate your predictions against the truth. Rememeber, this is a classification problem, so we need to use classification metrics here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:35:45.690311Z",
     "start_time": "2019-07-13T21:35:45.685311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4\n",
    "from sklearn.metrics.classification import classification_report, accuracy_score\n",
    "\n",
    "def report_performance(y_preds, y_test):\n",
    "    # TODO: implement this reporting function\n",
    "\n",
    "    print(classification_report(y_preds, y_test))\n",
    "    acc = accuracy_score(y_preds, y_test)\n",
    "    \n",
    "    print(f'accuracy: {acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Call your `report_performance` function to report the performance of your `nb` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:35:48.712484Z",
     "start_time": "2019-07-13T21:35:48.649481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.89      0.78      0.83      1812\n",
      "         HPL       0.75      0.90      0.82       944\n",
      "         MWS       0.81      0.83      0.82      1160\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      3916\n",
      "   macro avg       0.81      0.84      0.82      3916\n",
      "weighted avg       0.83      0.82      0.82      3916\n",
      "\n",
      "accuracy: 0.8227783452502554\n"
     ]
    }
   ],
   "source": [
    "# Step 5\n",
    "# TODO: report the performance of your `nb` classifier\n",
    "\n",
    "report_performance(y_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Omega$ 5: Train a Logistic Regression classifier\n",
    "\n",
    "In addition to the Naive Bayes classifier we just experimented with, we can also try the Logistic Regression model we learned so far. \n",
    "\n",
    "Again, we can use Sklearn's [Logistic Regression model](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Initialize a LogisticRegression object and call it `lr`. Train `lr` with the training data.\n",
    "\n",
    "> Hint: think about whether you need to set the `class_weight` argument when initializing the `LogisticRegression` object, base on your understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:36:13.912926Z",
     "start_time": "2019-07-13T21:36:13.565906Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valar Morghulis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Valar Morghulis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1\n",
    "# TODO: Initialize a LogisticRegression object and call it `lr`. Train `lr` with the training data.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Use the trained `lr` classifier to classify the texts in the test data. Report the performance of this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T21:36:21.351351Z",
     "start_time": "2019-07-13T21:36:21.260346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.83      0.80      0.81      1633\n",
      "         HPL       0.81      0.81      0.81      1133\n",
      "         MWS       0.79      0.82      0.80      1150\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      3916\n",
      "   macro avg       0.81      0.81      0.81      3916\n",
      "weighted avg       0.81      0.81      0.81      3916\n",
      "\n",
      "accuracy: 0.8097548518896833\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "# TODO: Use the trained lr classifier to classify the texts in the test data. \n",
    "# Report the performance of this classifier.\n",
    "\n",
    "y_preds = lr.predict(X_test)\n",
    "\n",
    "report_performance(y_preds, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
