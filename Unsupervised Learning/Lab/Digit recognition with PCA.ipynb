{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVM + PCA for Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 5, we have practiced using SVM to classify the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database). In this lab, we'll build upon the most vanilla SVM model and experiment with performing PCA dimension reduction on the data before training the model.\n",
    "\n",
    "Some brief review of the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:29:58.169863Z",
     "start_time": "2019-08-17T03:29:52.867729Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:29:58.177565Z",
     "start_time": "2019-08-17T03:29:58.173293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797 1797\n"
     ]
    }
   ],
   "source": [
    "print(len(digits.images), len(digits.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we are interested in is made of 8x8 images of digits, let's have a look at the first 10 images, stored in the `images` attribute of the dataset.  If we were working from image files, we could load them using `matplotlib.pyplot.imread`.  Note that each image must have the same size. For these images, we know which digit they represent: it is given in the `target` of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:30:00.454439Z",
     "start_time": "2019-08-17T03:30:00.146613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    plt.subplot(2, 10, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('%i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:30:01.049619Z",
     "start_time": "2019-08-17T03:30:01.045493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(digits.images)\n",
    "print(digits.images.shape)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class, we will demonstrate how to use PCA to preprocess our image data, and how to use sklearn's `Pipeline` to streamline the steps in training and predicting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will achieve the same results in two approaches:\n",
    "\n",
    "1. Manually chaining the following preprocessing steps and the actual model training step:\n",
    "    - Loading image data\n",
    "    - Flattening image pixels\n",
    "    - Standardizing pixel values\n",
    "    - Perform PCA\n",
    "    - Identify model\n",
    "    - Model fitting\n",
    "\n",
    "2. Taking advantage of sklearn's `Pipeline` class to easily chain steps together and set parameters for each step on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pipeline in sklearn?\n",
    "\n",
    "Definition of pipeline class according to scikit-learn is:\n",
    "\n",
    "> `Sequentially apply a list of transforms and a final estimator. Intermediate steps of pipeline must implement fit and transform methods and the final estimator only needs to implement fit.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:30:25.611865Z",
     "start_time": "2019-08-17T03:30:25.541627Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1797 images and 1797 labels.\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "print(f'Loaded {len(digits.images)} images and {len(digits.target)} labels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening image pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:30:26.167445Z",
     "start_time": "2019-08-17T03:30:26.035575Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "images_train, images_test, y_train, y_test = train_test_split(digits.images, digits.target, test_size=0.2, stratify=digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:30:26.641250Z",
     "start_time": "2019-08-17T03:30:26.637012Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(images):\n",
    "    \"\"\"\n",
    "    Input images is a (n_samples, 8, 8) matrix.\n",
    "    To apply a classifier on this data, we need to flatten the image, i.e.,\n",
    "    turn the data in a (samples, n_dim) matrix, where n_dim = 8*8\n",
    "\n",
    "    :param images: a 3D matrix with shape (n_samples, 8, 8)\n",
    "    :return: a flattened image matrix of shape (n_samples, 64)\n",
    "    \"\"\"\n",
    "    print(f'Shape before preprocessing: {images.shape}')\n",
    "    n_samples = images.shape[0]\n",
    "    data = images.reshape((n_samples, -1))\n",
    "\n",
    "    print(f'Shape after preprocessing: {data.shape}')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:30:27.393032Z",
     "start_time": "2019-08-17T03:30:27.389288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before preprocessing: (1437, 8, 8)\n",
      "Shape after preprocessing: (1437, 64)\n",
      "Shape before preprocessing: (360, 8, 8)\n",
      "Shape after preprocessing: (360, 64)\n"
     ]
    }
   ],
   "source": [
    "images_train = preprocess(images=images_train)\n",
    "images_test = preprocess(images=images_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to transform all inputs by using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:44:27.498892Z",
     "start_time": "2019-08-17T03:44:27.492111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "images_train_transformed = scaler.fit_transform(images_train)\n",
    "images_test_transformed = scaler.transform(images_test)\n",
    "images_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:44:28.440993Z",
     "start_time": "2019-08-17T03:44:28.425571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "\n",
    "images_train_transformed = pca.fit_transform(images_train_transformed)\n",
    "images_test_transformed = pca.fit_transform(images_test_transformed)\n",
    "images_test_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:52:32.733506Z",
     "start_time": "2019-08-17T03:52:32.730173Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a `pipeline` object by providing with the list of steps. Here our steps are standard scalar, PCA and support vector machine. **These steps are list of tuples consisting of name and an instance of the transformer or estimator.** Let’s see the piece of code below for clarification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:54:40.809570Z",
     "start_time": "2019-08-17T03:54:40.805824Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [('scaler', StandardScaler()), ('pca', PCA(n_components=50)), ('SVM', SVC(kernel='linear'))] # identify clearly the transformors and estimators.\n",
    "\n",
    "pipeline = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:54:41.347661Z",
     "start_time": "2019-08-17T03:54:41.340728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=50,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('SVM',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the parameters using the names issued. For instance, fit using n_components=30 for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:56:28.984078Z",
     "start_time": "2019-08-17T03:56:28.976730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=30,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('SVM',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.set_params(pca__n_components=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:57:50.877718Z",
     "start_time": "2019-08-17T03:57:50.807122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=30,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('SVM',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(images_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T03:58:29.608008Z",
     "start_time": "2019-08-17T03:58:29.593606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0, 2, 1, 2, 6, 3, 3, 2, 3, 4, 2, 2, 4, 1, 1, 3, 6, 0,\n",
       "       3, 9, 7, 7, 1, 5, 7, 4, 1, 8, 8, 1, 8, 1, 1, 3, 3, 2, 4, 7, 5, 6,\n",
       "       8, 7, 7, 0, 4, 2, 5, 1, 1, 5, 9, 2, 4, 3, 5, 2, 4, 0, 1, 8, 1, 2,\n",
       "       4, 6, 6, 5, 5, 8, 0, 4, 1, 5, 8, 2, 7, 6, 6, 6, 4, 5, 3, 8, 9, 9,\n",
       "       7, 3, 6, 3, 3, 7, 2, 6, 8, 4, 9, 6, 5, 2, 9, 5, 5, 4, 9, 9, 3, 2,\n",
       "       2, 4, 3, 1, 4, 6, 5, 0, 2, 9, 9, 2, 6, 2, 2, 5, 4, 1, 9, 4, 4, 4,\n",
       "       0, 3, 8, 9, 8, 5, 6, 1, 8, 5, 0, 1, 7, 7, 4, 8, 5, 2, 7, 7, 9, 9,\n",
       "       2, 8, 2, 2, 2, 9, 9, 8, 2, 8, 2, 3, 3, 8, 1, 5, 8, 3, 1, 1, 7, 6,\n",
       "       6, 9, 1, 8, 4, 2, 3, 7, 6, 8, 1, 4, 3, 3, 0, 4, 7, 5, 2, 6, 2, 2,\n",
       "       7, 5, 9, 4, 1, 2, 4, 5, 9, 8, 9, 3, 6, 2, 2, 1, 6, 7, 7, 6, 7, 2,\n",
       "       1, 6, 2, 2, 9, 8, 8, 2, 9, 2, 7, 4, 0, 6, 5, 4, 4, 5, 1, 0, 6, 1,\n",
       "       9, 0, 3, 8, 6, 4, 1, 2, 1, 5, 5, 1, 3, 3, 8, 6, 9, 2, 4, 6, 5, 9,\n",
       "       1, 1, 1, 9, 9, 1, 2, 1, 4, 3, 7, 9, 5, 9, 8, 7, 3, 1, 7, 5, 7, 8,\n",
       "       5, 1, 9, 3, 7, 2, 0, 5, 7, 8, 0, 6, 2, 6, 0, 7, 0, 8, 9, 2, 4, 6,\n",
       "       2, 0, 0, 3, 6, 2, 6, 5, 4, 3, 0, 7, 0, 3, 1, 9, 8, 2, 3, 5, 1, 0,\n",
       "       2, 8, 1, 3, 5, 7, 5, 6, 5, 8, 2, 1, 2, 1, 8, 1, 7, 1, 4, 3, 2, 4,\n",
       "       5, 2, 9, 6, 5, 2, 6, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(images_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why need pipeline?\n",
    "\n",
    "In a typical machine learning workflow you will need to apply all these transformations at least twice. Once when training the model and again on any new data you want to predict on. Of course you could write a function to apply them and reuse that but you would still need to run this first and then call the model separately. **Scikit-learn pipelines are a tool to simplify this process.** They have several key benefits:\n",
    "1. They make your workflow much easier to read and understand.\n",
    "2. They enforce the implementation and order of steps in your project.\n",
    "3. These in turn make your work much more reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "A pipeline can also be used during the model selection process. The following example code loops through a number of scikit-learn classifiers applying the transformations and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T04:17:52.632957Z",
     "start_time": "2019-08-17T04:17:44.954012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "model score: 0.153\n",
      "----\n",
      "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.092\n",
      "----\n",
      "NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "      kernel='rbf', max_iter=-1, nu=0.5, probability=True, random_state=None,\n",
      "      shrinking=True, tol=0.001, verbose=False)\n",
      "model score: 0.181\n",
      "----\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "model score: 0.183\n",
      "----\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "model score: 0.200\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jodie/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "model score: 0.133\n",
      "----\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "model score: 0.200\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=50)), ('classifier', classifier)]\n",
    "    pipe = Pipeline(steps)\n",
    "    pipe.fit(images_train_transformed, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(images_test_transformed, y_test))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
