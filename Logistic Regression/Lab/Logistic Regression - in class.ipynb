{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://weclouddata.com/wp-content/uploads/2016/11/logo.png' width='30%'>\n",
    "-------------\n",
    "\n",
    "<h3 align='center'> Applied Machine Learning Course - In-class Lab Week 2 </h3>\n",
    "<h1 align='center'> Logistic Regression </h1>\n",
    "\n",
    "<br>\n",
    "<center align=\"left\"> Developed by:</center>\n",
    "<center align=\"left\"> WeCloudData Academy </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Dataset\n",
    "\n",
    "In this exercise, we will be using the wine dataset to demonstrate properties of Logistic Regression.\n",
    "\n",
    "These data are the results of a **chemical analysis of wines grown in the same region in Italy** but derived from **3 different cultivars**. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \n",
    "\n",
    "The attributes are:\n",
    "1. Alcohol \n",
    "2. Malic acid \n",
    "3. Ash \n",
    "4. Alcalinity of ash \n",
    "5. Magnesium \n",
    "6. Total phenols \n",
    "7. Flavanoids \n",
    "8. Nonflavanoid phenols \n",
    "9. Proanthocyanins \n",
    "10. Color intensity \n",
    "11. Hue \n",
    "12. OD280/OD315 of diluted wines \n",
    "13. Proline \n",
    "\n",
    "In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.\n",
    "\n",
    "This is an exceedingly simple domain, but provides you with hands-on practice and understanding of how to build a classifier from scratch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine() # load the wine dataset\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "features = data.feature_names\n",
    "target_names = data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Practice : Explore the dataset\n",
    "\n",
    "You can take a look at the shape of the data ($X$), the features, and the target names.\n",
    "\n",
    "1. Print out the shape of the matrix $X$ and the vector $y$. You should expect to see $X.shape[0] == y.shape[0]$\n",
    "2. Print out the content of `features`. You should be seeing 13 individual features inside of it.\n",
    "3. Print out the content of `target_names`. How many classes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explore the dataset by completing the steps above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Delta$ 1. Visualize class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot alcohol vs. ash\n",
    "\n",
    "Let us choose two features and visualization the class distribution in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_indices = np.where(y == 0)[0] # all the sample indices with class=`class_0`\n",
    "class_0 = plt.scatter(X[class_0_indices,0], X[class_0_indices,1], c='b')\n",
    "\n",
    "class_1_indices = np.where(y == 1)[0] # all the sample indices with class=`class_1`\n",
    "class_1 = plt.scatter(X[class_1_indices,0], X[class_1_indices,1], c='r')\n",
    "\n",
    "class_2_indices = np.where(y == 2)[0] # all the sample indices with class=`class_2`\n",
    "class_2 = plt.scatter(X[class_2_indices,0], X[class_2_indices,1], c='g')\n",
    "\n",
    "plt.xlabel(\"alcohol\")\n",
    "plt.ylabel(\"ash\")\n",
    "plt.legend((class_0, class_1, class_2), (\"class_0\", \"class_1\", \"class_2\"))\n",
    "sns.despine() # remove the top and right spines from plot(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** Looking at the plot above, do you think the three classes can be well separated if we just use the two features `alcohol` and `ash` for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package your visualize function\n",
    "\n",
    "Let's make our visualization function more flexible. If you find yourself writing the same set of code multiple times, just with some small variable change, it's an indicator that you need to wrap them into one single function which you can call again and again later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d(feature_indices, all_feature_names, target_names, X, y):\n",
    "    series = []\n",
    "    # target_names=['class_0' 'class_1' 'class_2']\n",
    "    # targets=[0,1,2]\n",
    "    # assuming `feature_indices` contain two elements\n",
    "    idx1 = feature_indices[0]\n",
    "    idx2 = feature_indices[1]\n",
    "    colors = ['b', 'r', 'g']\n",
    "    for i in range(len(target_names)):\n",
    "        # for each target (\"class_0\", \"class_1\", \"class_2\")\n",
    "        # we create a scatter plot\n",
    "        target_indices = np.where(y == i)[0] # find the indices of data where the target == i\n",
    "        series.append(plt.scatter(X[target_indices, idx1], X[target_indices, idx2], c=colors[i]))\n",
    "        \n",
    "    plt.xlabel(all_feature_names[idx1]) # x-axis represents the first feature value\n",
    "    plt.ylabel(all_feature_names[idx2]) # y-axis represents the second feature value\n",
    "    plt.legend(series, target_names)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above can now be wrapped as one single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_2d(feature_indices=[features.index('alcohol'),\n",
    "                              features.index('ash')\n",
    "                             ], \n",
    "             all_feature_names=features,\n",
    "             target_names=target_names,\n",
    "             X=X,\n",
    "             y=y\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Practice: Visualize class distribution with \"color_intensity\" and \"hue\"\n",
    "\n",
    "Now let's perform a similar visualization as above, but with two different features `color_intensity` and `hue`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualizae class distribution among the dataset \n",
    "# by projecting the data points with two features only, \"color_intensity\" and \"hue\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Delta$ 2. Data preparation\n",
    "\n",
    "Now let's fit a **binary Logistic Regression classifier** on this set of data.\n",
    "\n",
    "Before doing so, we need to reduce our number of classes from 3 to 2.\n",
    "\n",
    "> Hint: we will merge the two classes which are linearly separable from the third into one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Merge `class_1` and `class_2` into one class, called `class_1_and_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: \n",
    "# 1. find the indices where y != 0 using np.where, and use these indices to reassign y\n",
    "# 2. modify target_names to reflect the change\n",
    "\n",
    "class_1_and_2_indices = np.where(y != 0)[0] # all the sample indices with class!=`1`\n",
    "target_names = ['class_0', 'class_1_and_2']\n",
    "y[class_1_and_2_indices] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ (Practice)  Step 2:  Count how many `class_0` and `class_1_and_2` there are in your transformed data\n",
    "\n",
    "This step is useful for checking whether the dataset you are dealing with is a balanced one or a highly-skewed one. Highly skewed dataset is particularly challenging for classification, as the classifier would tend to ignore minority class when performing optimization, and result in those minority class performing really poorly in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dimension reduction (a rough one)\n",
    "\n",
    "Select the two features out of the total 13 and project the dataset onto the two chosen features. Apply this dimension reduction `X_train` and `X_test`.\n",
    "\n",
    "> We are performing a very rudimentory dimension reduction here, simply by choosing two (random) features out of all possible features and discard the rest. In reality, dimension reduction should be performed in a more disciplined manner, which is a topic in the **Advanced Machine Learning** course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [2, 6] # or some others as your wish \n",
    "X = X[:, np.array(feature_indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Step 4:  (Practice) Visualize your new data points with the same visualize_2d function\n",
    "\n",
    "> Be careful with the `feature_indices` this time, since your `X` is already reduced to 2 dimensions only.\n",
    "\n",
    "**Question**: Does this new dataset, which contain two classes only, is linearly separable by your chosen features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: visualize your new dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Step 5:  (Practice) Split the entire dataset to be 80% training and 20% test (validation)\n",
    "\n",
    "Recall that, in order to evaluate our trained model, we need to always reserve a fraction of our dataset as the validation set, so we can obtain evaluation metrics by comparing the ground-truth labels in this validation set and the predictions generated by our model.\n",
    "\n",
    "We performed the same splitting in the Lab notebook in Week 1.\n",
    "\n",
    "Hint: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Step 6: (Practice) Verify the size of the two datasets: \n",
    " - `X_train` and `y_train`: the training set\n",
    " - `X_test` and `y_test`: the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Standardize your training data\n",
    "\n",
    "As introduced in the Lab notebook of Week 1, in general, it is a good idea to perform certain preprocessing steps, such as feature standardization and normalization, imputation, outlier removal, on your training data.\n",
    "\n",
    "Here, we perform the same feature standardization as we did in Week 1.\n",
    "\n",
    "Hint: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Step 8: Standardize your test data\n",
    "\n",
    "Now we need to **use the same scaler** to **transform** our test data. \n",
    "\n",
    "<font color='#FF0000'>Be careful, don't call `fit_transform` again on the test data!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the same scaler (fit on the training data) to transform our test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\Delta$ 3. Model fitting\n",
    "\n",
    "- We will be using Scikit-learn's `LogisticRegression` module to perform our **binary logistic regression** classification. As a challenge, you can try implementing your own **gradient descent optimization** on logistic regression, and we leave the practice in this week's assignment.\n",
    "\n",
    "- **User guide:** http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is this `class_weight` parameter? Why do we want to set it to `balanced`?\n",
    "\n",
    "**Hint**: read the documentation about the `class_weight` paramter of `LogisticRegression` on http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression and reference to the class distribution issue mentioned in the section **(Practice) Step 2: Count how many class_0 and class_1_and_2 there are in your transformed data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit the model using `X_train` and `y_train`\n",
    "\n",
    "When using sklearn models, such as the `LinearRegression` model we used in Week 1 and the `LogisticRegression` model we use here, training a model is as simple as one line code: `model.fit(X, y)`.\n",
    "\n",
    "The reason behind this simplicity is due to the good software engineering principles implemented in sklearn, such as encapsulation and polymorphism (so it provides a consistent interface to users, regardless of the underlying model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use `lr` to generate the predictions on `X_train`\n",
    "\n",
    "Normally, we do not need to generate predictions on the training data, as we care about the model's ability to generate predictions on unseen data.\n",
    "\n",
    "However, here, let's generate predictions on the training data anyway, so we can compare the training performance against the validation performance, to get a sense of whether we are overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the model performance on the training set\n",
    "\n",
    "Looking at training performance is a good way of see if your model is underfitting or not, i.e., if the model cannot even fit the training data almost perfectly, it indicates your model is not expressive enough to explain all the variance in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print('Training performance')\n",
    "print(classification_report(y_pred_train, y_train))\n",
    "accuracy = accuracy_score(y_pred_train, y_train) *100\n",
    "print(\"Classification accuracy: {}%.\".format(round(accuracy,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ Step 5: (Practice) Evaluate the model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate your model's performance on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\Omega$ (Practice) Manually calculate the test performance metrics using `y_test` and `y_pred_test`\n",
    "\n",
    "By comparing the ground truth `y_test` with our predictions `y_pred_test`, we can compute the set of metrics we got above: `preicision`, `recall`, `f1-score`, and `accuracy`.\n",
    "\n",
    "In this exercise, we will achieve this without using sklearn's function. This is to help you establish a solid understanding of how those metrics work.\n",
    "\n",
    "Verify your calculation by confirming your hand calculated results are the same as the ones we got above.\n",
    "\n",
    "> Be careful of how you define your `positive` class, which might be different with what is defined by sklearn when it generates classification report.\n",
    "> Hint: you can get the positive class defined by sklearn with this line of code:\n",
    "> `pos_class = lr.classes_[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TP (true positive)\n",
    "\n",
    "# Compute FP (false positive)\n",
    "\n",
    "# Compute FN (false negative)\n",
    "\n",
    "# Compute TN (true negative)\n",
    "\n",
    "# Compute precision\n",
    "\n",
    "# Compute recall\n",
    "\n",
    "# Compute f1-score\n",
    "\n",
    "# Compute accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get prediction probabilities\n",
    "\n",
    "As mentioned, one of the advantages of using Logistic Regression is the ability to product natural prediction probabilities. By default, when you call `lr.predict()`, it uses probability $\\ge 0.5$ as the threshold for producing positive predictions, and output the categorical labels directly for you.\n",
    "\n",
    "Now let's take a look at how these probabilities actually look like. We call the `.predict_prob()` function on a trained LogisticRegression model to get the raw probabilities for each output class.\n",
    "\n",
    "> <font color='#FF0000'>Note, not all classifiers implement `.predict_prob()`, because not all classifiers have a valid probility interpretation in their output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = lr.predict_proba(X_test) # `predict_prob` is the way to get the predictions in raw probabilities\n",
    "classes = lr.classes_\n",
    "pos_idx = list(classes).index(1)  # find the class index representing the positive class `1`\n",
    "\n",
    "for (i, y) in enumerate(y_test):\n",
    "    prob = test_probs[i, pos_idx] # get the probability P(C=1|X) for the i-th test sample\n",
    "    print('true class: %d, predicted prob: %.2f' % (y_test[i], round(prob, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize the decision boundary\n",
    "\n",
    "We can also visualize what the decision boundary look like for our trained LogisticRegression model. The following code might seem a bit too complicated, but the gist of it is to plot out the contour when the prediction probilities equal to 0.5, as in the following line:\n",
    "\n",
    "`plt.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary_2d(target_names, X, y, lr):\n",
    "    series = []\n",
    "    # target_names=['class_0', 'class_1_and_2']\n",
    "    # targets=[0,1]\n",
    "    colors = ['b', 'r', 'g']\n",
    "    for i in range(len(target_names)):\n",
    "        # for each target (\"class_0\", \"class_1_and_2\")\n",
    "        # we create a scatter plot\n",
    "        target_indices = np.where(y == i)[0] # find the indices of data where the target == i\n",
    "        series.append(plt.scatter(X[target_indices, 0], X[target_indices, 1], c=colors[i]))\n",
    " \n",
    "    xx, yy = np.mgrid[0:1:.01, 0:1:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = lr.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "    plt.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    " \n",
    "    plt.legend(series, target_names)\n",
    "   \n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decision_boundary_2d(target_names=['class_0', 'class_1_and_2'], X=X_train, y=y_train, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Precision-recall tradeoff\n",
    "\n",
    "Since Logistic Regression is able to predict probabilities naturally, we can theoretically change the default threshold (0.5) of separating postivie (1) vs. negative (0) classes.\n",
    "\n",
    "Let's see how changing this threshold affect our metrics.\n",
    "\n",
    "The following code plots the relationship between **precision** and **recall** when the decision threshold is changed. What pattern can you observe from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.classification import precision_score, recall_score\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "thresholds = np.linspace(0, 0.9) # evenly 10 spaced numbers over [0, 0.9].\n",
    "classes = lr.classes_\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "pos_idx = list(classes).index(1)  # find the class index representing the positive class `1`\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "for threshold in thresholds:\n",
    "    \n",
    "    pos_j = np.where(test_probs[:,pos_idx] >= threshold) # the index in the test set whose prediction is above the threshold\n",
    "    y_preds = np.zeros(len(y_test))\n",
    "    y_preds[pos_j[0]] = 1\n",
    "\n",
    "    precisions.append(precision_score(y_preds, y_test))\n",
    "    recalls.append(recall_score(y_preds, y_test))\n",
    "\n",
    "plt.plot(thresholds, precisions, c='r') # the precision curve\n",
    "plt.plot(thresholds, recalls, c='g') # the recall curve\n",
    "\n",
    "y = np.linspace(0, 1, 100)\n",
    "plt.plot([0.5]*len(y), y, linestyle='--', linewidth=1, c='k') # the default threshold 0.5\n",
    "\n",
    "plt.xlabel('threshold')\n",
    "prec_patch = mpatches.Patch(color='red', label='Precision')\n",
    "rec_patch = mpatches.Patch(color='green', label='Recall')\n",
    "plt.legend(handles=[prec_patch, rec_patch])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Recall**: among all the truly Yes cases, how many of them are predicted by us.\n",
    "\n",
    "> **Precision:** among all the predicted Yes cases, how many of them truly Yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
