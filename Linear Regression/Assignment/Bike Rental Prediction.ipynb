{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bike Rental Prediction.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fVJRwI5E_vmr","colab_type":"text"},"source":["<img src='https://weclouddata.com/wp-content/uploads/2016/11/logo.png' width='30%'>\n","-------------\n","\n","<h3 align='center'> Applied Machine Learning Course - Assignment Week 1 </h3>\n","<h1 align='center'> Bike Rental Prediction </h1>\n","\n","<br>\n","<center align=\"left\"> Developed by:</center>\n","<center align=\"left\"> WeCloudData Academy </center>\n"]},{"cell_type":"markdown","metadata":{"id":"Zt_fVPOc_vmx","colab_type":"text"},"source":["<h2>Background</h2>\n","\n","Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n","\n","The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n","\n","You are provided daily rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each day covered by the test set, using only information available prior to the rental period.\n","\n","> We will be using the `train.csv` for this assignment."]},{"cell_type":"markdown","metadata":{"id":"UQU7FBek_vm0","colab_type":"text"},"source":["<h2>Data Description</h2>\n","\n","<b>Features:</b>\n","\n","- datetime - hourly date + timestamp\n","- season -  1 = spring, 2 = summer, 3 = fall, 4 = winter\n","- holiday - whether the day is considered a holiday\n","- workingday - whether the day is neither a weekend nor holiday\n","- weather  \n","  - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n","  - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n","  - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n","  - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n","\n","- temp - temperature in Celsius\n","- atemp - \"feels like\" temperature in Celsius\n","- humidity - relative humidity\n","- windspeed - wind speed\n","\n","<b>Features should not be used:</b>\n","\n","- casual - number of non-registered user rentals initiated (Not Provided in Test Data)\n","- registered - number of registered user rentals initiated (Not Provided in Test Data)\n","\n","<b>Target Value:</b>\n","\n","- count - number of total rentals"]},{"cell_type":"markdown","metadata":{"id":"BafmiX9T_vm4","colab_type":"text"},"source":["## $\\Omega$ 1: Explore the Training Data"]},{"cell_type":"markdown","metadata":{"id":"Yts-34Wi_vm7","colab_type":"text"},"source":["- Step 1: Import two libraries: \n","  - 'pandas', \n","  - 'numpy'\n","  - 'matplotlib'(used for data visualization)\n","\n","- Step 2: Load the training data `train.csv` into a Dataframe named 'data'.\n","\n","- Step 3: Explore the dataframe"]},{"cell_type":"code","metadata":{"id":"e2hJgtlN_vm_","colab_type":"code","colab":{}},"source":["#Step 1\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GB3Rw9Ig_vnN","colab_type":"code","colab":{}},"source":["#Step 2\n","data = pd.read_csv('')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xa9To3U_vnV","colab_type":"code","colab":{}},"source":["#Step 3\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwOzhWCY_vnb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAsyWG4__vng","colab_type":"text"},"source":["## $\\Omega$ 2: Get Dummy Variables"]},{"cell_type":"markdown","metadata":{"id":"_abjb1fQ_vnh","colab_type":"text"},"source":["- <b>Step 1: Drop the `casual` and `registered` columns.</b>"]},{"cell_type":"code","metadata":{"id":"eFfq0ToI_vnj","colab_type":"code","colab":{}},"source":["# Step 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Blz3w76P_vnn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Optb9wsu_vnr","colab_type":"text"},"source":["- <b>Step 2: Covert the 'datetime' feature to Year, Month, and Hour</b>\n","  - Date attribute cannot be taken as a numerical feature in a regression analysis. We need to convert the 'datetime' column into three columns: Year, Month, and hour.\n","  - example: 2011-01-01 02:00:00 \n","    - Year: 2011, \n","    - Month: 1, \n","    - Hour: 2\n","\n"," - After add those three columns, drop the 'datetime' column."]},{"cell_type":"code","metadata":{"id":"ZEXX_btt_vns","colab_type":"code","colab":{}},"source":["# Step 2\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eqmxw5_O_vnv","colab_type":"text"},"source":["- <b>Step 3: Get a list of all categorical variables</b>\n","\n","  - Use data.columns to check what columns in our `data` now.\n","\n","  - Now that in our data we have both numerical features and categorical features, we need to convert categorical features to Numerical features.\n","\n","  - Some of the features are already 0,1 (binary) style, such as `holiday`. So we don't need to convert them to dummy variables.\n","\n","  - In step 3, please identify Categorical features (those need to be converted) in the data and save all their column names to a list.\n","    - Example: `categorical_features = ['season', ...]`"]},{"cell_type":"code","metadata":{"id":"4wXeXBUB_vnx","colab_type":"code","colab":{},"outputId":"203624fb-f2da-494f-a5c3-183d3663b800"},"source":["# Step 3\n","data.columns"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b8522876bc61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"8VJCm2Ub_vn3","colab_type":"code","colab":{}},"source":["categorical_features = []"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9xgNAXo_vn5","colab_type":"text"},"source":["- <b>Step 4: Converting categorical features to numerical </b>\n","  - Convert these categorical_features to dummies\n","  - Hint:\n","    - `data = pd.get_dummies(data,columns=categorical_features,drop_first=True)`\n","    - Guess what 'drop_first=True' means here and why we need it."]},{"cell_type":"code","metadata":{"id":"VOZufp2r_vn6","colab_type":"code","colab":{}},"source":["# Step 4\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SSuiodB_vn9","colab_type":"text"},"source":["- <b>Step 5: Count how many columns in the Dataset</b>"]},{"cell_type":"code","metadata":{"id":"xZW0WS58_vn-","colab_type":"code","colab":{}},"source":["# Step 5\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k0AfwxN_voA","colab_type":"text"},"source":["## $\\Omega$ 3: Prepare Training and Testing Data"]},{"cell_type":"markdown","metadata":{"id":"7Sn8NRVL_voD","colab_type":"text"},"source":["- <b>Step 1</b>\n","\n","  - According to the Data Information, our Target Value is data['count'].\n","  - Input Features should include all other features **except**: data['count'], data['casual'], data['registered']\n","  - So we need to set y = data['count'] and X include other part of the dataframe except those three columns we just mentioned."]},{"cell_type":"code","metadata":{"id":"AOkzT2Ni_voE","colab_type":"code","colab":{}},"source":["# Step 1\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"toh69C-T_voG","colab_type":"text"},"source":["- <b>Step 2</b>\n","\n","  - Use the 'train_test_split' function in scikit learn to split X and y into 80% Traning data and 20% Testing Data\n","  \n","  - Hint: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"]},{"cell_type":"code","metadata":{"id":"h7FuORSD_voH","colab_type":"code","colab":{}},"source":["# Step 2\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2MvH9ML_voK","colab_type":"text"},"source":["- <b>Step 3</b>\n","\n","  - Perform feature standardization on `X_train` by using sklearn's `StandardScaler`, and use the same standardizer to standardize `X_test`.\n","  - Hint: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"]},{"cell_type":"code","metadata":{"id":"udl_7yX1_voL","colab_type":"code","colab":{}},"source":["# Step 3\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yw9GMCGi_voP","colab_type":"text"},"source":["## $\\Omega$ 4: Linear Regression </h3>"]},{"cell_type":"markdown","metadata":{"id":"h3R6MFI8_voQ","colab_type":"text"},"source":["- <b>Step 1</b>\n","\n","  - Import the linear_model from scikit learn and mean_squared_error to evaluate the result of the regression model \n","  - Create a Linear Regression model - 'lr' and fit X_train and y_train to train it.\n","  - Hint: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"]},{"cell_type":"code","metadata":{"id":"G3z6-J2m_voS","colab_type":"code","colab":{}},"source":["#Step 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jojmL1lI_voW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSHREIi1_voZ","colab_type":"text"},"source":["- <b>Step 2</b>\n","  - Use lr.predict on X_test to get predicted value of y and call mean_squared_error(Y_test, y_predict) to get the mean squre error(MSE).\n","  - Call numpy's function to calculate the squre root of MSE(RMSE)."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"xo1Lor2E_voa","colab_type":"code","colab":{}},"source":["# Step 2\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rY8SKHca_vod","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LuY5BJgT_vof","colab_type":"text"},"source":["## $\\Omega$ Problem 5 (Advanced): Implement gradient descent on linear regression</h3>"]},{"cell_type":"markdown","metadata":{"id":"yh3AggEm_vog","colab_type":"text"},"source":["- <b>Step 1</b>\n","\n","  - Under the hood, Scikit-learn does not use gradient descent to fit LinearRegression; rather, it uses an approximation to the exact analytical solution we have seen using calculus.\n","\n","  - Therefore, it is a chanllenge for you to implement gradient descent on linear regession.\n","\n","  - In this step, we initialize a weight/coefficient vector $\\theta$ randomly. This vector should have the same dimensionality of the features in the training data, i.e., one coefficient for one single feature. "]},{"cell_type":"code","metadata":{"id":"i1Tr_cNH_vog","colab_type":"code","colab":{}},"source":["# Step 1\n","def initialize_theta(dim):\n","    # TODO: randomly initialize coefficient to be a dim-sized 1D vector\n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Clwtjfb_voj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FY_0bNoz_vol","colab_type":"text"},"source":["- <b>Step 2</b>\n","\n","  - Calculate the gradient of the linear regresion function $h_\\theta(x)$ against each $\\theta_i$. Hint: we have talked about this gradient in the lecture."]},{"cell_type":"code","metadata":{"id":"g23zTQ3U_vol","colab_type":"code","colab":{}},"source":["# Step 2: implement this function\n","def prediction(X, current_theta):\n","    # TODO: compute the current estimation of the output, H, given the current_theta and X\n","    \n","def loss(X, y, current_theta):\n","    # TODO: compute loss function J, given H, and y\n","    \n","def loss_gradient(X, y, current_theta):\n","    # TODO: implement the loss gradient\n","    # compute the current estimation of the output, H, given the current_theta and X\n","    # compute gradient of the loss function against current_theta\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mifbHfW__von","colab_type":"text"},"source":["- <b>Step 3</b>\n","\n","  - Define a learning rate $\\alpha$, which would be the step size to update each $\\theta_i$, $\\theta_i=\\theta_i - \\alpha \\times loss\\_gradient_i$. Repeatedly updating all $\\theta_i$'s until the loss converges."]},{"cell_type":"code","metadata":{"id":"fiUnEXwV_voo","colab_type":"code","colab":{}},"source":["# Step 3:\n","\n","def update_theta(gradient, current_theta, step_size):\n","    # TODO: implement theta update logic\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEjLtAyE_voq","colab_type":"code","colab":{}},"source":["# We put the skeleton code here for you\n","\n","# initialization:\n","# convert pandas dataframe into numpy arrays\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","X_test = np.array(X_test)\n","y_test = np.array(y_test)\n","\n","precision = 0.001\n","step_size = 0.1 # use your own step_size\n","current_theta = initialize_theta(dim=) # you need to determine the input value to `dim`\n","current_loss = loss(X_train, y_train, current_theta)\n","losses = [current_loss]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9rYWnoH_vot","colab_type":"code","colab":{}},"source":["# main graident descent loop\n","while len(losses) < 2 or abs(losses[-1] - losses[-2]) > precision: # all some other convergence condition\n","    gradient = loss_gradient(X_train, y_train, current_theta)\n","    current_theta = update_theta(gradient, current_theta, step_size)\n","    \n","    # compute current loss\n","    current_loss = loss(X_train, y_train, current_theta)\n","    losses.append(current_loss)\n","    \n","# once converged, current_theta are therefore the coefficients in the linear regression model  \n","print(f'converge after {len(losses)} iterations')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwS5SyJ2_vov","colab_type":"code","colab":{}},"source":["# plot loss against number of iterations\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CZuQYktA_voy","colab_type":"text"},"source":["- <b>Step 4</b>\n","\n","  - Calculate the RMSE of your trained model on test data"]},{"cell_type":"code","metadata":{"id":"UbOYrRnw_voz","colab_type":"code","colab":{}},"source":["# Step 4\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"twgf15N2_vo1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}